{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fh = np.load('../data/dataset.npz')\n",
    "# We have a bunch of feature columns and last column is the y-target\n",
    "train_x = fh['train_x'].astype(np.int64)\n",
    "train_y = fh['train_y']\n",
    "\n",
    "test_x = fh['test_x'].astype(np.int64)\n",
    "test_y = fh['test_y']\n",
    "\n",
    "n_user = int(fh['n_user'])\n",
    "n_item = int(fh['n_item'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the MF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def l2_regularize(array):\n",
    "    loss = torch.sum(array ** 2.0)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class MF(nn.Module):\n",
    "    itr = 0\n",
    "    \n",
    "    def __init__(self, n_user, n_item, k=18, c=4,\n",
    "                 c_vector=1.0, c_bias=1.0, writer=None):\n",
    "        super(MF, self).__init__()\n",
    "        self.writer = writer\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # **NEW: user taste & attention vectors\n",
    "        user_taste = torch.zeros(n_user, k, c)\n",
    "        user_attnd = torch.zeros(n_user, k, c)\n",
    "        user_taste.data.normal_(0, 1.0 / n_user)\n",
    "        user_attnd.data.normal_(0, 1.0 / n_user)\n",
    "        \n",
    "        self.user_taste = nn.Parameter(user_taste)\n",
    "        self.user_attnd = nn.Parameter(user_attnd)\n",
    "\n",
    "    \n",
    "    def __call__(self, train_x):\n",
    "        user_id = train_x[:, 0]\n",
    "        item_id = train_x[:, 1]\n",
    "        vector_item = self.item(item_id)\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        \n",
    "        # **NEW: user taste & attention\n",
    "        user_taste = self.user_taste[user_id]\n",
    "        user_attnd = self.user_attnd[user_id]\n",
    "        vector_itemx = vector_item.unsqueeze(2).expand_as(user_attnd)\n",
    "        attention = F.softmax(user_attnd * vector_itemx)\n",
    "        attentionx = attention.sum(2).unsqueeze(2).expand_as(user_attnd)\n",
    "        weighted_preference = (user_taste * attentionx).sum(2)\n",
    "        dot = (weighted_preference * vector_item).sum(1)\n",
    "        \n",
    "        prediction = dot + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        loss_mse = F.mse_loss(prediction.squeeze(), target.squeeze())\n",
    "        prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        prior_taste =  l2_regularize(self.user_taste) * self.c_vector\n",
    "        prior_attnd =  l2_regularize(self.user_attnd) * self.c_vector\n",
    "        prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        \n",
    "        total = (loss_mse + prior_bias_item + prior_bias_user +\n",
    "                 prior_taste + prior_attnd + prior_item)\n",
    "        for name, var in locals().items():\n",
    "            if type(var) is torch.Tensor and var.nelement() == 1 and self.writer is not None:\n",
    "                self.writer.add_scalar(name, var, self.itr)\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Loss\n",
    "from tensorboardX import SummaryWriter\n",
    "from ignite.metrics import MeanSquaredError\n",
    "\n",
    "from loader import Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "k = 10\n",
    "c = 4\n",
    "c_bias = 1e-6\n",
    "c_vector = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MF(\n",
       "  (bias_user): Embedding(6041, 1)\n",
       "  (bias_item): Embedding(3953, 1)\n",
       "  (item): Embedding(3953, 10)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "model = MF(n_user, n_item,  k=k, c=c, c_bias=c_bias, \n",
    "           c_vector=c_vector, writer=writer)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "trainer = create_supervised_trainer(model, optimizer, model.loss)\n",
    "metrics = {'accuracy': MeanSquaredError()}\n",
    "evaluat = create_supervised_evaluator(model, metrics=metrics)\n",
    "train_loader = Loader(train_x, train_y, batchsize=1024)\n",
    "test_loader = Loader(test_x, test_y, batchsize=1024)\n",
    "\n",
    "\n",
    "def log_training_loss(engine, log_interval=400):\n",
    "    epoch = engine.state.epoch\n",
    "    itr = engine.state.iteration\n",
    "    fmt = \"Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n",
    "    msg = fmt.format(epoch, itr, len(train_loader), engine.state.output)\n",
    "    model.itr = itr\n",
    "    if itr % log_interval == 0:\n",
    "        print(msg)\n",
    "\n",
    "trainer.add_event_handler(event_name=Events.ITERATION_COMPLETED, handler=log_training_loss)\n",
    "\n",
    "def log_validation_results(engine):\n",
    "    evaluat.run(test_loader)\n",
    "    metrics = evaluat.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    print(\"Epoch[{}] Validation MSE: {:.2f} \"\n",
    "          .format(engine.state.epoch, avg_accuracy))\n",
    "    writer.add_scalar(\"validation/avg_accuracy\", avg_accuracy, engine.state.epoch)\n",
    "\n",
    "trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=log_validation_results)\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(train_loader, max_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
