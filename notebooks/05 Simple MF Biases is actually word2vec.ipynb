{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "codes = np.load(\"../data/skipgram_full.npz\")['coded']\n",
    "code2token = np.load(\"../data/skipgram_full.npz\")['c2t'].tolist()\n",
    "token2code = np.load(\"../data/skipgram_full.npz\")['t2c'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  16,  570,   19],\n",
       "       [4299,  570,   26],\n",
       "       [8099, 6605,    2],\n",
       "       ...,\n",
       "       [6645, 3386,    1],\n",
       "       [8619, 7315,    1],\n",
       "       [3250, 5845,    1]], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First column is the first token code\n",
    "# second column is the 2nd token code\n",
    "# third column is the skip gram count\n",
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = codes[:, :2].copy().astype(np.int64)\n",
    "train_y = np.log(codes[:, 2]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_user = np.max(train_x[:, :2]) + 1\n",
    "n_item = np.max(train_x[:, :2]) + 1\n",
    "n_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the MF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def l2_regularize(array):\n",
    "    loss = torch.sum(array ** 2.0)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class MF(nn.Module):\n",
    "    itr = 0\n",
    "    \n",
    "    def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0, writer=None):\n",
    "        super(MF, self).__init__()\n",
    "        self.writer = writer\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        model.user.weight.data.normal_(0, 1.0 / n_user)\n",
    "        model.item.weight.data.normal_(0, 1.0 / n_item)\n",
    "        \n",
    "        # We've added new terms here:\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    \n",
    "    def __call__(self, train_x):\n",
    "        user_id = train_x[:, 0]\n",
    "        item_id = train_x[:, 1]\n",
    "        vector_user = self.user(user_id)\n",
    "        vector_item = self.item(item_id)\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        prediction = ui_interaction + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        prior_bias_user = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        total = loss_mse + prior_user + prior_item\n",
    "        for name, var in locals().items():\n",
    "            if type(var) is torch.Tensor and var.nelement() == 1 and self.writer is not None:\n",
    "                self.writer.add_scalar(name, var, self.itr)\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Loss\n",
    "from tensorboardX import SummaryWriter\n",
    "from ignite.metrics import MeanSquaredError\n",
    "\n",
    "from loader import Loader\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs/simple_mf_05_word2vec_2018-08-21_14:48:27.068048\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "k = 128\n",
    "c_bias = 1e-6\n",
    "c_vector = 1e-6\n",
    "log_dir = 'runs/simple_mf_05_word2vec_' + str(datetime.now()).replace(' ', '_')\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MF(\n",
       "  (user): Embedding(10000, 128)\n",
       "  (item): Embedding(10000, 128)\n",
       "  (bias_user): Embedding(10000, 1)\n",
       "  (bias_item): Embedding(10000, 1)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "model = MF(n_user, n_item,  k=k, c=c, c_bias=c_bias, \n",
    "           c_vector=c_vector, writer=writer)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "trainer = create_supervised_trainer(model, optimizer, model.loss)\n",
    "metrics = {'accuracy': MeanSquaredError()}\n",
    "train_loader = Loader(train_x, train_y, batchsize=1024)\n",
    "\n",
    "\n",
    "def log_training_loss(engine, log_interval=400):\n",
    "    epoch = engine.state.epoch\n",
    "    itr = engine.state.iteration\n",
    "    fmt = \"Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n",
    "    msg = fmt.format(epoch, itr, len(train_loader), engine.state.output)\n",
    "    model.itr = itr\n",
    "    if itr % log_interval == 0:\n",
    "        print(msg)\n",
    "\n",
    "trainer.add_event_handler(event_name=Events.ITERATION_COMPLETED, handler=log_training_loss)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model_05_word2vec\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Iteration[400/22215] Loss: 0.46\n",
      "Epoch[1] Iteration[800/22215] Loss: 0.46\n",
      "Epoch[1] Iteration[1200/22215] Loss: 0.46\n",
      "Epoch[1] Iteration[1600/22215] Loss: 0.44\n",
      "Epoch[1] Iteration[2000/22215] Loss: 0.48\n",
      "Epoch[1] Iteration[2400/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[2800/22215] Loss: 0.48\n",
      "Epoch[1] Iteration[3200/22215] Loss: 0.48\n",
      "Epoch[1] Iteration[3600/22215] Loss: 0.49\n",
      "Epoch[1] Iteration[4000/22215] Loss: 0.46\n",
      "Epoch[1] Iteration[4400/22215] Loss: 0.47\n",
      "Epoch[1] Iteration[4800/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[5200/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[5600/22215] Loss: 0.52\n",
      "Epoch[1] Iteration[6000/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[6400/22215] Loss: 0.52\n",
      "Epoch[1] Iteration[6800/22215] Loss: 0.46\n",
      "Epoch[1] Iteration[7200/22215] Loss: 0.47\n",
      "Epoch[1] Iteration[7600/22215] Loss: 0.55\n",
      "Epoch[1] Iteration[8000/22215] Loss: 0.47\n",
      "Epoch[1] Iteration[8400/22215] Loss: 0.53\n",
      "Epoch[1] Iteration[8800/22215] Loss: 0.53\n",
      "Epoch[1] Iteration[9200/22215] Loss: 0.47\n",
      "Epoch[1] Iteration[9600/22215] Loss: 0.51\n",
      "Epoch[1] Iteration[10000/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[10400/22215] Loss: 0.55\n",
      "Epoch[1] Iteration[10800/22215] Loss: 0.49\n",
      "Epoch[1] Iteration[11200/22215] Loss: 0.51\n",
      "Epoch[1] Iteration[11600/22215] Loss: 0.55\n",
      "Epoch[1] Iteration[12000/22215] Loss: 0.47\n",
      "Epoch[1] Iteration[12400/22215] Loss: 0.49\n",
      "Epoch[1] Iteration[12800/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[13200/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[13600/22215] Loss: 0.57\n",
      "Epoch[1] Iteration[14000/22215] Loss: 0.51\n",
      "Epoch[1] Iteration[14400/22215] Loss: 0.51\n",
      "Epoch[1] Iteration[14800/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[15200/22215] Loss: 0.53\n",
      "Epoch[1] Iteration[15600/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[16000/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[16400/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[16800/22215] Loss: 0.47\n",
      "Epoch[1] Iteration[17200/22215] Loss: 0.45\n",
      "Epoch[1] Iteration[17600/22215] Loss: 0.54\n",
      "Epoch[1] Iteration[18000/22215] Loss: 0.49\n",
      "Epoch[1] Iteration[18400/22215] Loss: 0.49\n",
      "Epoch[1] Iteration[18800/22215] Loss: 0.50\n",
      "Epoch[1] Iteration[19200/22215] Loss: 0.49\n"
     ]
    }
   ],
   "source": [
    "trainer.run(train_loader, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_05_word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "label_token = ['|' + code2token[c] for c in range(n_user)]\n",
    "writer.add_embedding(model.user.weight, metadata=label_token)\n",
    "writer.add_embedding(model.item.weight, metadata=label_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspect the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate what urban dictionary thinks are similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_raw = model.item.weights.get()\n",
    "vectors = vectors_raw / (vectors_raw**2.0).sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(token):\n",
    "    code = token2code[token]\n",
    "    vector = vectors[code]\n",
    "    similarity = np.dot(vector[None, :], vector)\n",
    "    closest = np.argsort(similarity)\n",
    "    for code in closest:\n",
    "        print(code2token[code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('yolo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('barbie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('relationship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('pope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('blinding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('conk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('doofer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('earwig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('fuzz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('honk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('ivories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('paddy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('tosh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract and add word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subtract(center, minus, plus):\n",
    "    vector = (vectors[token2code[center]]\n",
    "             - vectors[token2code[minus]]\n",
    "             + vectors[token2code[plus]])\n",
    "    similarity = np.dot(vector[None, :], vector)\n",
    "    closest = np.argsort(similarity)\n",
    "    for code in closest:\n",
    "        print(code2token[code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_subtract('cop', 'fuzz', 'cookie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_subtract('cop', 'fuzz', 'crib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
